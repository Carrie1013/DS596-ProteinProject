{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def call_psipred_api(sequence: str):\n",
    "    psipred = \"http://bioinf.cs.ucl.ac.uk/psipred/api\"\n",
    "    submit_url = f\"{psipred}/submission\"\n",
    "    fasta_sequence = f\">query\\n{sequence}\"\n",
    "\n",
    "    payload = {'input_data': fasta_sequence}\n",
    "    data = {'job': 'psipred', 'submission_name': 'test','email': 'carrief0908@gmail.com'}\n",
    "    r = requests.post(f\"{submit_url}.json\", data=data, files=payload)\n",
    "    response_data = json.loads(r.text)\n",
    "    print(response_data)\n",
    "    uuid = response_data['UUID']\n",
    "\n",
    "    retries = 0\n",
    "    while retries < 30:\n",
    "      result_uri = f\"{submit_url}/{uuid}\"\n",
    "      r = requests.get(result_uri, headers={\"Accept\":\"application/json\"})\n",
    "      result_data = json.loads(r.text)\n",
    "      if \"Complete\" in result_data[\"state\"]:\n",
    "          data_path = result_data['submissions'][0]['results'][5]['data_path']\n",
    "          response = requests.get(f\"{psipred}{data_path}\")\n",
    "          if response.status_code != 200:\n",
    "              raise Exception(f\"Failed to get results: {response.text}\")\n",
    "          ss_sequence = \"\"\n",
    "          for line in response.text.splitlines():\n",
    "              if not line.startswith('#') and len(line.split()) > 2:\n",
    "                  ss_sequence += line.split()[2]\n",
    "          return ss_sequence\n",
    "      else:\n",
    "          retries += 1\n",
    "          time.sleep(30)\n",
    "\n",
    "    raise Exception(\"Timeout waiting for PSIPRED results\")\n",
    "\n",
    "\n",
    "\n",
    "split_data = pd.read_csv('../data/split.csv')\n",
    "split_data = split_data[['Split Site', 'Sequence']]\n",
    "split_data['Sequence'] = split_data['Sequence'].str.replace(' ', '', regex=False)\n",
    "\n",
    "expanded_rows = split_data['Split Site'].str.split('/').explode()\n",
    "expanded_data = pd.DataFrame({\n",
    "    'Split Site': expanded_rows,\n",
    "    'Sequence': split_data.loc[expanded_rows.index, 'Sequence'].values\n",
    "})\n",
    "expanded_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seq_json(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "Seqs = read_seq_json('../data/seq_2_second.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data['Secondary'] = expanded_data['Sequence'].map(Seqs)\n",
    "expanded_data.dropna(subset=[\"Secondary\"], inplace=True) # 换成AlphaFold结果之后记得删掉这行\n",
    "expanded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_data.to_csv('expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded_data_grouped = expanded_data.groupby(['Sequence', 'Secondary'])['Split Site'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'expanded.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "unique_sequences = expanded_data['Sequence'].unique()\n",
    "expanded_data_ = []\n",
    "\n",
    "for sequence in unique_sequences:\n",
    "    \n",
    "    seq_data = expanded_data[expanded_data['Sequence'] == sequence]\n",
    "    split_sites = seq_data['Split Site'].tolist()\n",
    "    secondary_structure = seq_data['Secondary'].iloc[0]\n",
    "    \n",
    "    max_site = max(split_sites)\n",
    "    all_sites = list(range(1, max_site + 1))\n",
    "    for site in all_sites:\n",
    "        expanded_data_.append({\n",
    "            'Site': site,\n",
    "            'Split': site in split_sites,\n",
    "            'Sequence': sequence,\n",
    "            'Secondary': secondary_structure\n",
    "        })\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'expanded.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cq1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
