{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/qiaochufeng/Documents/GitHub/DS596-Project\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from proteinbert import load_pretrained_model\n",
    "\n",
    "\n",
    "BENCHMARK_NAME = 'secondary_structure'\n",
    "BENCHMARKS_DIR = 'protein_bert/protein_benchmarks'\n",
    "\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.train.csv')\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.1, random_state=0)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.test.csv')\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024_11_26-22:01:29] 3 unique lebels.\n",
      "[2024_11_26-22:01:30] Training set: Filtered out 499 of 7810 (6.4%) records of lengths exceeding 510.\n",
      "[2024_11_26-22:01:30] Validation set: Filtered out 71 of 868 (8.2%) records of lengths exceeding 510.\n",
      "[2024_11_26-22:01:30] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float32' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m model_generator \u001b[38;5;241m=\u001b[39m FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     23\u001b[0m         get_model_with_hidden_layers_as_outputs, dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     25\u001b[0m training_callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.25\u001b[39m, min_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-05\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     27\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, restore_best_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     28\u001b[0m ]\n\u001b[0;32m---> 30\u001b[0m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_SPEC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs_per_stage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-04\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin_with_frozen_pretrained_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_with_frozen_pretrained_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-02\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_final_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_seq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_lr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtraining_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/protein_bert-1.0.1-py3.8.egg/proteinbert/finetuning.py:52\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, seq_len, batch_size, max_epochs_per_stage, lr, begin_with_frozen_pretrained_layers, lr_with_frozen_pretrained_layers, n_final_epochs, final_seq_len, final_lr, callbacks)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m begin_with_frozen_pretrained_layers:\n\u001b[1;32m     51\u001b[0m     log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining with frozen pretrained layers...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mmodel_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_valid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs_per_stage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_with_frozen_pretrained_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_pretrained_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining the entire fine-tuned model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m model_generator\u001b[38;5;241m.\u001b[39mtrain(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr \u001b[38;5;241m=\u001b[39m lr, callbacks \u001b[38;5;241m=\u001b[39m callbacks, \\\n\u001b[1;32m     57\u001b[0m         freeze_pretrained_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/protein_bert-1.0.1-py3.8.egg/proteinbert/model_generation.py:27\u001b[0m, in \u001b[0;36mModelGenerator.train\u001b[0;34m(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr, callbacks, **create_model_kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(seq_len, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcreate_model_kwargs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m \u001b[38;5;241m=\u001b[39m lr\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_X, train_Y, sample_weight \u001b[38;5;241m=\u001b[39m train_sample_weigths, batch_size \u001b[38;5;241m=\u001b[39m batch_size, epochs \u001b[38;5;241m=\u001b[39m n_epochs, validation_data \u001b[38;5;241m=\u001b[39m encoded_valid_set, \\\n\u001b[1;32m     30\u001b[0m         callbacks \u001b[38;5;241m=\u001b[39m callbacks)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_state(model)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:1013\u001b[0m, in \u001b[0;36mOptimizerV2.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hyper\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper:\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_hyper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1015\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(name, value)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:918\u001b[0m, in \u001b[0;36mOptimizerV2._set_hyper\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyper[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hyper\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/keras/src/backend.py:4277\u001b[0m, in \u001b[0;36mset_value\u001b[0;34m(x, value)\u001b[0m\n\u001b[1;32m   4275\u001b[0m value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(value, dtype\u001b[38;5;241m=\u001b[39mdtype_numpy(x))\n\u001b[1;32m   4276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[0;32m-> 4277\u001b[0m     \u001b[43m_assign_value_to_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4279\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mcm/lib/python3.8/site-packages/keras/src/backend.py:4361\u001b[0m, in \u001b[0;36m_assign_value_to_variable\u001b[0;34m(variable, value)\u001b[0m\n\u001b[1;32m   4358\u001b[0m     variable\u001b[38;5;241m.\u001b[39massign(d_value)\n\u001b[1;32m   4359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4360\u001b[0m     \u001b[38;5;66;03m# For the normal tf.Variable assign\u001b[39;00m\n\u001b[0;32m-> 4361\u001b[0m     \u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m(value)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float32' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len, log\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "\n",
    "output_type = OutputType(True, 'categorical')\n",
    "if output_type.is_categorical:\n",
    "    if output_type.is_seq:\n",
    "        unique_labels = sorted(set.union(*train_set['label'].apply(set)) | set.union(*valid_set['label'].apply(set)) | \\\n",
    "                set.union(*test_set['label'].apply(set)))\n",
    "    else:\n",
    "        unique_labels = sorted(set(train_set['label'].unique()) | set(valid_set['label'].unique()) | set(test_set['label'].unique()))\n",
    "    log('%d unique lebels.' % len(unique_labels))\n",
    "elif output_type.is_binary:\n",
    "    unique_labels = [0, 1]\n",
    "else:\n",
    "    unique_labels = None\n",
    "OUTPUT_SPEC = OutputSpec(output_type, unique_labels)\n",
    "\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
    "]\n",
    "\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
    "        seq_len = 512, batch_size = 32, max_epochs_per_stage = 1, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 411ms/step\n"
     ]
    }
   ],
   "source": [
    "test_seqs_truncated = [seq[:510] for seq in test_set['seq']]\n",
    "\n",
    "X = input_encoder.encode_X(test_seqs_truncated, 512)\n",
    "model = pretrained_model_generator.create_model(512)\n",
    "y_pred = model.predict(X, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 512, 26)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Blast import NCBIWWW\n",
    "\n",
    "sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.StringIO at 0x1115f4160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = esmfold.infer_pdb(sequence)\n",
    "\n",
    "pdb_filename = \"/Users/qiaochufeng/Downloads/6kl9.pdb\"\n",
    "with open(pdb_filename, \"w\") as f:\n",
    "    f.write(output)\n",
    "\n",
    "pdb_file = PDBFile.read(pdb_filename)\n",
    "structure = pdb_file.get_structure()\n",
    "backbone = structure[(structure.atom_name == \"N\") | \n",
    "                     (structure.atom_name == \"CA\") | \n",
    "                     (structure.atom_name == \"C\")]\n",
    "\n",
    "secondary_structure = struc.annotate_sse(backbone)\n",
    "\n",
    "sse_symbols = ''.join(secondary_structure)\n",
    "print(\"predicted structure:\")\n",
    "print(sse_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_psipred_api(sequence: str):\n",
    "    psipred = \"http://bioinf.cs.ucl.ac.uk/psipred/api\"\n",
    "    submit_url = f\"{psipred}/submission\"\n",
    "    fasta_sequence = f\">query\\n{sequence}\"\n",
    "\n",
    "    payload = {'input_data': fasta_sequence}\n",
    "    data = {'job': 'psipred', 'submission_name': 'test','email': 'carrief0908@gmail.com'}\n",
    "    r = requests.post(f\"{submit_url}.json\", data=data, files=payload)\n",
    "    response_data = json.loads(r.text)\n",
    "    print(response_data)\n",
    "    uuid = response_data['UUID']\n",
    "\n",
    "    retries = 0\n",
    "    while retries < 30:\n",
    "      result_uri = f\"{submit_url}/{uuid}\"\n",
    "      r = requests.get(result_uri, headers={\"Accept\":\"application/json\"})\n",
    "      result_data = json.loads(r.text)\n",
    "      if \"Complete\" in result_data[\"state\"]:\n",
    "          data_path = result_data['submissions'][0]['results'][5]['data_path']\n",
    "          response = requests.get(f\"{psiprid}{data_path}\")\n",
    "          if response.status_code != 200:\n",
    "              raise Exception(f\"Failed to get results: {response.text}\")\n",
    "          ss_sequence = \"\"\n",
    "          for line in response.text.splitlines():\n",
    "              if not line.startswith('#') and len(line.split()) > 2:\n",
    "                  ss_sequence += line.split()[2]\n",
    "          return ss_sequence\n",
    "      else:\n",
    "          retries += 1\n",
    "          time.sleep(30)\n",
    "\n",
    "    raise Exception(\"Timeout waiting for PSIPRED results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UUID': '68045620-aced-11ef-979d-00163e100466', 'submission_name': 'test'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CCCCHHHCCCCCEEEEEEEEEECCEEEEEEEEEEECCCCEEEEEEEEEECCCCCCCHHHHHHHCCCCCEECCCCCCCCCCCCHHHHHCCCCCEEEEEEEEECCCEEEEEEEEEEECCEEEEEEEEEEECCCCCCCCCCCCCCCCCCCCCEEEEECCCCCCEEEEEEEEEECCCCCEEEEEEECCCCCCCCCCCCCCCCEEEEEEEEECCCCCCCCCCEEEEEEHHHCCCCCCCCCCCC'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "def call_psipred_api(sequence: str):\n",
    "    psipred = \"http://bioinf.cs.ucl.ac.uk/psipred/api\"\n",
    "    submit_url = f\"{psipred}/submission\"\n",
    "    fasta_sequence = f\">query\\n{sequence}\"\n",
    "\n",
    "    payload = {'input_data': fasta_sequence}\n",
    "    data = {'job': 'psipred', 'submission_name': 'test','email': 'carrief0908@gmail.com'}\n",
    "    r = requests.post(f\"{submit_url}.json\", data=data, files=payload)\n",
    "    response_data = json.loads(r.text)\n",
    "    print(response_data)\n",
    "    uuid = response_data['UUID']\n",
    "\n",
    "    retries = 0\n",
    "    while retries < 30:\n",
    "      result_uri = f\"{submit_url}/{uuid}\"\n",
    "      r = requests.get(result_uri, headers={\"Accept\":\"application/json\"})\n",
    "      result_data = json.loads(r.text)\n",
    "      if \"Complete\" in result_data[\"state\"]:\n",
    "          data_path = result_data['submissions'][0]['results'][5]['data_path']\n",
    "          response = requests.get(f\"{psipred}{data_path}\")\n",
    "          if response.status_code != 200:\n",
    "              raise Exception(f\"Failed to get results: {response.text}\")\n",
    "          ss_sequence = \"\"\n",
    "          for line in response.text.splitlines():\n",
    "              if not line.startswith('#') and len(line.split()) > 2:\n",
    "                  ss_sequence += line.split()[2]\n",
    "          return ss_sequence\n",
    "      else:\n",
    "          retries += 1\n",
    "          time.sleep(30)\n",
    "\n",
    "    raise Exception(\"Timeout waiting for PSIPRED results\")\n",
    "\n",
    "call_psipred_api(\"MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
