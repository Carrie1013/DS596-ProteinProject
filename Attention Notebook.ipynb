{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d88e930-27e4-40e3-bcb7-1e47aa2b8418",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mproteinbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_model\n\u001b[1;32m      9\u001b[0m BENCHMARK_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfluorescence\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m BENCHMARKS_DIR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotein_bert/protein_benchmarks\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:664\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:627\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen zipimport>:259\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/protein_bert-1.0.1-py3.9.egg/proteinbert/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ADDED_TOKENS_PER_SEQ\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_generation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelGenerator, PretrainingModelGenerator, FinetuningModelGenerator, InputEncoder, load_pretrained_model_from_dump, tokenize_seqs\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexisting_model_loading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pretrained_model\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinetuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputType, OutputSpec, finetune, evaluate_by_len\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:664\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:627\u001b[0m, in \u001b[0;36m_load_backward_compatible\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen zipimport>:259\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/protein_bert-1.0.1-py3.9.egg/proteinbert/model_generation.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m additional_token_to_index, n_tokens, tokenize_seq\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelGenerator\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer_class \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-04\u001b[39m, other_optimizer_kwargs \u001b[38;5;241m=\u001b[39m {}, model_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, optimizer_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class \u001b[38;5;241m=\u001b[39m optimizer_class\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/protein_bert-1.0.1-py3.9.egg/proteinbert/model_generation.py:13\u001b[0m, in \u001b[0;36mModelGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelGenerator\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer_class \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241m.\u001b[39mAdam, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2e-04\u001b[39m, other_optimizer_kwargs \u001b[38;5;241m=\u001b[39m {}, model_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, optimizer_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class \u001b[38;5;241m=\u001b[39m optimizer_class\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:182\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_initialized:\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfll_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\n\u001b[1;32m    186\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m   ):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:182\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_initialized:\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfll_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\n\u001b[1;32m    186\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m   ):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m     )\n",
      "    \u001b[0;31m[... skipping similar frames: KerasLazyLoader.__getattr__ at line 182 (2953 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:182\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_initialized:\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfll_keras_version\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras_3\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    183\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_submodule\n\u001b[1;32m    186\u001b[0m       \u001b[38;5;129;01mand\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompat.v1.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m   ):\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` is not available with Keras 3. Keras 3 has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno support for TF 1 APIs. You can install the `tf_keras` package \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.compat.v1.keras` to `tf_keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/tensorflow/python/util/lazy_loader.py:178\u001b[0m, in \u001b[0;36mKerasLazyLoader.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 178\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_tfll_mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_tfll_initialized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_tfll_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(types\u001b[38;5;241m.\u001b[39mModuleType, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)\n\u001b[1;32m    180\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfll_initialized:\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from proteinbert import load_pretrained_model\n",
    "\n",
    "\n",
    "BENCHMARK_NAME = 'fluorescence'\n",
    "BENCHMARKS_DIR = 'protein_bert/protein_benchmarks'\n",
    "\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.train.csv')\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.1, random_state=0)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, f'{BENCHMARK_NAME}.test.csv')\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde30424-50b6-4186-b1e6-1700be72d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model_encoder\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "def encode_sequences_with_merge(sequences, encoder, seq_len=512):\n",
    "    encoded = encoder.encode_X(sequences, seq_len=seq_len)\n",
    "    valid_encoded = encoded[0] if isinstance(encoded, list) else encoded\n",
    "    return valid_encoded\n",
    "\n",
    "train_features = encode_sequences_with_merge(train_set['seq'], input_encoder, seq_len=512)\n",
    "valid_features = encode_sequences_with_merge(valid_set['seq'], input_encoder, seq_len=512)\n",
    "test_features = encode_sequences_with_merge(test_set['seq'], input_encoder, seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70acf9f2-a0d1-492d-8835-4ff671210007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731521904.716278   33581 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m604/604\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 63.6400 - mean_squared_error: 63.6400 - val_loss: 0.7316 - val_mean_squared_error: 0.7316\n",
      "Epoch 2/20\n",
      "\u001b[1m604/604\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3986 - mean_squared_error: 1.3986 - val_loss: 0.8557 - val_mean_squared_error: 0.8557\n",
      "Epoch 3/20\n",
      "\u001b[1m604/604\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3475 - mean_squared_error: 1.3475 - val_loss: 0.7390 - val_mean_squared_error: 0.7390\n",
      "Epoch 4/20\n",
      "\u001b[1m604/604\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2774 - mean_squared_error: 1.2774 - val_loss: 0.7760 - val_mean_squared_error: 0.7760\n",
      "\u001b[1m851/851\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563us/step - loss: 1.9027 - mean_squared_error: 1.9027\n",
      "Test Loss (MSE): 1.8927136659622192, Test MSE: 1.8927136659622192\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "def build_mlp(input_dim, output_dim=1):\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(output_dim, activation='linear')  # Regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "mlp_model = build_mlp(input_dim=train_features.shape[1])\n",
    "\n",
    "# train the MLP\n",
    "mlp_model.fit(\n",
    "    train_features, train_set['label'],\n",
    "    validation_data=(valid_features, valid_set['label']),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_loss, test_mse = mlp_model.evaluate(test_features, test_set['label'], batch_size=32)\n",
    "print(f'Test Loss (MSE): {test_loss}, Test MSE: {test_mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1cc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4459351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be95f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d396f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94be008b-470a-44df-8f1f-30520ab6f762",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3111aa670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Number of attention layers: 6\n",
      "Shape of last attention layer output: (1, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected attention shape: (1, 512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m     ax\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTokens\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Load model and process\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m attention_matrix, seq_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_attentions_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     50\u001b[0m plot_attention_fixed(attention_matrix, seq_tokens, ax)\n",
      "Cell \u001b[0;32mIn[16], line 28\u001b[0m, in \u001b[0;36mcalculate_attentions_direct\u001b[0;34m(model, input_encoder, seq, seq_len)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attention_matrix, seq_tokens\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected attention shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_attention_layer\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected attention shape: (1, 512)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cd612-6470-4469-a9de-64d3c0f07cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74319fb-ffca-4cfa-86d5-f45bc2c02840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1227e2-b8e5-4b03-8f0b-8b39bfc08da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606efec-17e2-47b5-aded-9478a21322dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cbbe8b09-22bf-49fa-bbe5-b96ba53fdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def custom_calculate_attention(layer, inputs):\n",
    "\n",
    "    # X: (batch_size, d_global_input)\n",
    "    # S: (batch_size, length, d_seq_input)\n",
    "    X, S = inputs\n",
    "    _, length, _ = K.int_shape(S)\n",
    "    QX = K.tanh(K.dot(X, layer.Wq))\n",
    "    Q = K.tanh(K.dot(S, layer.Wk))\n",
    "    \n",
    "    attention_scores = K.batch_dot(Q, K.permute_dimensions(QX, (0, 2, 1)))\n",
    "    attention_scores = K.softmax(attention_scores)\n",
    "    \n",
    "    return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "137858ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m seq \u001b[38;5;241m=\u001b[39m test_set\u001b[38;5;241m.\u001b[39mloc[chosen_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     42\u001b[0m label \u001b[38;5;241m=\u001b[39m test_set\u001b[38;5;241m.\u001b[39mloc[chosen_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_attentions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[55], line 19\u001b[0m, in \u001b[0;36mcalculate_attentions\u001b[0;34m(model, input_encoder, seq, seq_len)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalAttention\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer)):\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39minput\n\u001b[0;32m---> 19\u001b[0m         ly \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_calculate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         model_attentions\u001b[38;5;241m.\u001b[39mappend(ly)\n\u001b[1;32m     23\u001b[0m invoke_model_attentions \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mfunction(model_inputs, model_attentions)\n",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m, in \u001b[0;36mcustom_calculate_attention\u001b[0;34m(layer, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m X, S \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Ensure dynamic shape works properly\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Dynamic batch size\u001b[39;00m\n\u001b[1;32m     15\u001b[0m length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(S)[\u001b[38;5;241m1\u001b[39m]      \u001b[38;5;66;03m# Sequence length\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Compute QX and Q using the learned weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "def calculate_attentions(model, input_encoder, seq, seq_len = None):\n",
    "    \n",
    "    from tensorflow.keras import backend as K\n",
    "    from proteinbert.tokenization import index_to_token\n",
    "    \n",
    "    if seq_len is None:\n",
    "        seq_len = len(seq) + 2\n",
    "    \n",
    "    X = input_encoder.encode_X([seq], seq_len)\n",
    "    (X_seq,), _ = X\n",
    "    seq_tokens = list(map(index_to_token.get, X_seq))\n",
    "\n",
    "    model_inputs = [layer.input for layer in model.layers if 'InputLayer' in str(type(layer))][::-1]\n",
    "    model_attentions = []\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'GlobalAttention' in str(type(layer)):\n",
    "            input = layer.input\n",
    "            ly = custom_calculate_attention(layer, input)\n",
    "            model_attentions.append(ly)\n",
    "\n",
    "\n",
    "    invoke_model_attentions = K.function(model_inputs, model_attentions)\n",
    "    attention_values = invoke_model_attentions(X)\n",
    "    \n",
    "    attention_labels = []\n",
    "    merged_attention_values = []\n",
    "\n",
    "    for attention_layer_index, attention_layer_values in enumerate(attention_values):\n",
    "        for head_index, head_values in enumerate(attention_layer_values):\n",
    "            attention_labels.append('Attention %d - head %d' % (attention_layer_index + 1, head_index + 1))\n",
    "            merged_attention_values.append(head_values)\n",
    "\n",
    "    attention_values = np.array(merged_attention_values)\n",
    "\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "model = pretrained_model_generator.create_model(seq_len=512)\n",
    "\n",
    "chosen_index = ((test_set['seq'].str.len() - 80).abs()).sort_values().index[0]\n",
    "seq = test_set.loc[chosen_index, 'seq']\n",
    "label = test_set.loc[chosen_index, 'label']\n",
    "\n",
    "pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels = calculate_attentions(model, input_encoder, seq, seq_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b72208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention_values, seq_tokens, attention_labels, ax, cmap = 'Reds', vmin = 0, vmax = None, text_value_threshold = 0.1):\n",
    "\n",
    "    heatmap = ax.pcolor(attention_values.transpose(), cmap = cmap, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(attention_labels)) + 0.5)\n",
    "    ax.set_xticklabels(attention_labels, rotation = 45, ha = 'right', fontsize = 12)\n",
    "    ax.set_yticks(np.arange(len(seq_tokens)) + 0.5)\n",
    "    ax.set_yticklabels(seq_tokens, fontsize = 12)\n",
    "\n",
    "    for i, row in enumerate(attention_values):\n",
    "        for j, value in enumerate(row):\n",
    "            if abs(value) >= text_value_threshold:\n",
    "                add_plus_sign = attention_values.min() < 0 and value > 0\n",
    "                plus_sign = '+' if add_plus_sign else ''\n",
    "                ax.text(i + 0.5, j + 0.5, plus_sign + '%d%%' % (100 * value), color = 'white', ha = 'center', va = 'center', \\\n",
    "                        fontsize = 9, fontweight = 'bold', fontstretch = 'condensed')\n",
    "                \n",
    "                \n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_attention(pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels, ax, cmap='Reds')\n",
    "plt.title(\"Attention Visualization - Pretrained Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07888c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'rank'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m pretrained_model_generator, input_encoder \u001b[38;5;241m=\u001b[39m load_pretrained_model()\n\u001b[1;32m     63\u001b[0m model \u001b[38;5;241m=\u001b[39m pretrained_model_generator\u001b[38;5;241m.\u001b[39mcreate_model(seq_len)\n\u001b[0;32m---> 64\u001b[0m pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_attentions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m model \u001b[38;5;241m=\u001b[39m model_generator\u001b[38;5;241m.\u001b[39mcreate_model(seq_len)\n\u001b[1;32m     68\u001b[0m finetuned_attention_values, finetuned_seq_tokens, finetuned_attention_labels \u001b[38;5;241m=\u001b[39m calculate_attentions(model, input_encoder, seq, \\\n\u001b[1;32m     69\u001b[0m         seq_len \u001b[38;5;241m=\u001b[39m seq_len)\n",
      "Cell \u001b[0;32mIn[56], line 22\u001b[0m, in \u001b[0;36mcalculate_attentions\u001b[0;34m(model, input_encoder, seq, seq_len)\u001b[0m\n\u001b[1;32m     19\u001b[0m seq_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(index_to_token\u001b[38;5;241m.\u001b[39mget, X_seq))\n\u001b[1;32m     21\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39minput \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInputLayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer))][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m model_attentions \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mcalculate_attention(layer\u001b[38;5;241m.\u001b[39minput) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalAttention\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer))]\n\u001b[1;32m     23\u001b[0m invoke_model_attentions \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mfunction(model_inputs, model_attentions)\n\u001b[1;32m     24\u001b[0m attention_values \u001b[38;5;241m=\u001b[39m invoke_model_attentions(X)\n",
      "Cell \u001b[0;32mIn[56], line 22\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m seq_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(index_to_token\u001b[38;5;241m.\u001b[39mget, X_seq))\n\u001b[1;32m     21\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39minput \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInputLayer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer))][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 22\u001b[0m model_attentions \u001b[38;5;241m=\u001b[39m [\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalAttention\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer))]\n\u001b[1;32m     23\u001b[0m invoke_model_attentions \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mfunction(model_inputs, model_attentions)\n\u001b[1;32m     24\u001b[0m attention_values \u001b[38;5;241m=\u001b[39m invoke_model_attentions(X)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/protein_bert-1.0.1-py3.9.egg/proteinbert/conv_and_global_attention_model.py:72\u001b[0m, in \u001b[0;36mGlobalAttention.calculate_attention\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m _, length, _ \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mint_shape(S)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, d_key)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m QX \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWq\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# (batch_size * n_heads, d_key)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m QX_batched_heads \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mreshape(QX, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_key))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/keras/src/legacy/backend.py:792\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.dot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(x, y):\n\u001b[1;32m    791\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (ndim(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ndim(y) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    793\u001b[0m         x_shape \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, tf\u001b[38;5;241m.\u001b[39munstack(tf\u001b[38;5;241m.\u001b[39mshape(x))):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cq1/lib/python3.9/site-packages/keras/src/legacy/backend.py:1055\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.ndim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mndim\u001b[39m(x):\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BENCHMARK_DISPLAY_NAME = 'Signal peptide'\n",
    "\n",
    "TEST_SET_FILE_PATH = 'protein_benchmarks/signalP_binary.train.csv'\n",
    "IDEAL_LEN = 80\n",
    "\n",
    "def calculate_attentions(model, input_encoder, seq, seq_len = None):\n",
    "    \n",
    "    from tensorflow.keras import backend as K\n",
    "    from proteinbert.tokenization import index_to_token\n",
    "    \n",
    "    if seq_len is None:\n",
    "        seq_len = len(seq) + 2\n",
    "    \n",
    "    X = input_encoder.encode_X([seq], seq_len)\n",
    "    (X_seq,), _ = X\n",
    "    seq_tokens = list(map(index_to_token.get, X_seq))\n",
    "\n",
    "    model_inputs = [layer.input for layer in model.layers if 'InputLayer' in str(type(layer))][::-1]\n",
    "    model_attentions = [layer.calculate_attention(layer.input) for layer in model.layers if 'GlobalAttention' in str(type(layer))]\n",
    "    invoke_model_attentions = K.function(model_inputs, model_attentions)\n",
    "    attention_values = invoke_model_attentions(X)\n",
    "    \n",
    "    attention_labels = []\n",
    "    merged_attention_values = []\n",
    "\n",
    "    for attention_layer_index, attention_layer_values in enumerate(attention_values):\n",
    "        for head_index, head_values in enumerate(attention_layer_values):\n",
    "            attention_labels.append('Attention %d - head %d' % (attention_layer_index + 1, head_index + 1))\n",
    "            merged_attention_values.append(head_values)\n",
    "\n",
    "    attention_values = np.array(merged_attention_values)\n",
    "    \n",
    "    return attention_values, seq_tokens, attention_labels\n",
    "\n",
    "def plot_attention(attention_values, seq_tokens, attention_labels, ax, cmap = 'Reds', vmin = 0, vmax = None, text_value_threshold = 0.1):\n",
    "\n",
    "    heatmap = ax.pcolor(attention_values.transpose(), cmap = cmap, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(attention_labels)) + 0.5)\n",
    "    ax.set_xticklabels(attention_labels, rotation = 45, ha = 'right', fontsize = 12)\n",
    "    ax.set_yticks(np.arange(len(seq_tokens)) + 0.5)\n",
    "    ax.set_yticklabels(seq_tokens, fontsize = 12)\n",
    "\n",
    "    for i, row in enumerate(attention_values):\n",
    "        for j, value in enumerate(row):\n",
    "            if abs(value) >= text_value_threshold:\n",
    "                add_plus_sign = attention_values.min() < 0 and value > 0\n",
    "                plus_sign = '+' if add_plus_sign else ''\n",
    "                ax.text(i + 0.5, j + 0.5, plus_sign + '%d%%' % (100 * value), color = 'white', ha = 'center', va = 'center', \\\n",
    "                        fontsize = 9, fontweight = 'bold', fontstretch = 'condensed')\n",
    "                \n",
    "test_set = pd.read_csv(TEST_SET_FILE_PATH)\n",
    "chosen_index = ((test_set['seq'].str.len() - IDEAL_LEN).abs()).sort_values().index[0]\n",
    "seq = test_set.loc[chosen_index, 'seq']\n",
    "label = test_set.loc[chosen_index, 'label']\n",
    "                \n",
    "seq_len = len(seq) + 2\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "model = pretrained_model_generator.create_model(seq_len)\n",
    "pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels = calculate_attentions(model, input_encoder, seq, \\\n",
    "        seq_len = seq_len)\n",
    "\n",
    "model = model_generator.create_model(seq_len)\n",
    "finetuned_attention_values, finetuned_seq_tokens, finetuned_attention_labels = calculate_attentions(model, input_encoder, seq, \\\n",
    "        seq_len = seq_len)\n",
    "assert finetuned_seq_tokens == pretrained_seq_tokens\n",
    "assert finetuned_attention_labels == pretrained_attention_labels[:len(finetuned_attention_labels)]\n",
    "\n",
    "fig, axes = plt.subplots(ncols = 4, figsize = (20, 0.2 * seq_len), gridspec_kw = dict(width_ratios = [1, 5, 1, 5]))\n",
    "fig.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "axes[0].barh(np.arange(seq_len), 100 * pretrained_attention_values.sum(axis = 0), color = '#EC7063')\n",
    "axes[0].set_ylim((-0.5, seq_len - 0.5))\n",
    "axes[0].set_yticks([])\n",
    "axes[0].invert_xaxis()\n",
    "axes[0].set_xlabel('Total atten. %', fontsize = 14)\n",
    "\n",
    "vmax = pretrained_attention_values.max()\n",
    "plot_attention(pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels, axes[1], cmap = 'Reds', vmax = vmax, \\\n",
    "        text_value_threshold = 0.05)\n",
    "axes[1].set_title('Only pre-training', fontsize = 16)\n",
    "\n",
    "axes[2].barh(np.arange(seq_len), 100 * (finetuned_attention_values - pretrained_attention_values).sum(axis = 0), color = '#28B463')\n",
    "axes[2].set_ylim((-0.5, seq_len - 0.5))\n",
    "axes[2].set_yticks([])\n",
    "axes[2].invert_xaxis()\n",
    "axes[2].set_xlabel('Total atten. % diff', fontsize = 14)\n",
    "\n",
    "attention_diff = finetuned_attention_values - pretrained_attention_values[:len(finetuned_attention_labels), :]\n",
    "vmax = np.abs(attention_diff).max()\n",
    "plot_attention(attention_diff, finetuned_seq_tokens, finetuned_attention_labels, axes[3], cmap = 'PiYG', vmin = -vmax, vmax = vmax, \\\n",
    "        text_value_threshold = 0.03)\n",
    "axes[3].set_title('%s fine-tuning' % BENCHMARK_DISPLAY_NAME, fontsize = 16)\n",
    "\n",
    "print(seq, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1382da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
